I. üå± T·ªïng quan MongoDB
Th√†nh ph·∫ßn	M√¥ t·∫£
MongoDB	CSDL NoSQL d·∫°ng document (l∆∞u tr·ªØ JSON/BSON).
Collection	T·∫≠p h·ª£p c√°c document, t∆∞∆°ng t·ª± table trong SQL.
Document	M·ªôt b·∫£n ghi (record), d·∫°ng JSON.
Replica Set	C·ª•m HA g·ªìm 1 primary v√† ‚â•2 secondary (replication).
Sharding	Ph√¢n m·∫£nh d·ªØ li·ªáu theo shard key ƒë·ªÉ scale horizontal.
mongod	Daemon ch√≠nh c·ªßa MongoDB.
mongos	Query router khi s·ª≠ d·ª•ng sharding.
mongo shell / mongosh	CLI ƒë·ªÉ truy v·∫•n v√† c·∫•u h√¨nh.
II. üß± C√†i ƒë·∫∑t MongoDB tr√™n Linux (Ubuntu / CentOS)
1Ô∏è‚É£ Ubuntu 22.04
sudo apt update
sudo apt install gnupg curl -y
curl -fsSL https://pgp.mongodb.com/server-7.0.asc | sudo gpg -o /usr/share/keyrings/mongodb-server-7.gpg --dearmor
echo "deb [ signed-by=/usr/share/keyrings/mongodb-server-7.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list
sudo apt update
sudo apt install -y mongodb-org
sudo systemctl enable mongod
sudo systemctl start mongod
sudo systemctl status mongod

2Ô∏è‚É£ CentOS / RHEL
cat <<EOF | sudo tee /etc/yum.repos.d/mongodb-org.repo
[mongodb-org-7.0]
name=MongoDB Repository
baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/7.0/x86_64/
gpgcheck=1
enabled=1
gpgkey=https://www.mongodb.org/static/pgp/server-7.0.asc
EOF
sudo yum install -y mongodb-org
sudo systemctl enable --now mongod

III. ‚öôÔ∏è C·∫•u h√¨nh c∆° b·∫£n
File c·∫•u h√¨nh /etc/mongod.conf
storage:
  dbPath: /var/lib/mongo
  journal:
    enabled: true
systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
net:
  port: 27017
  bindIp: 0.0.0.0
security:
  authorization: enabled
replication:
  replSetName: "rs0"


bindIp = 0.0.0.0 ƒë·ªÉ l·∫Øng nghe t·∫•t c·∫£ interfaces (ch·ªâ d√πng trong n·ªôi b·ªô).

authorization: enabled ‚Üí y√™u c·∫ßu login b·∫±ng user/password.

replication.replSetName k√≠ch ho·∫°t ch·∫ø ƒë·ªô Replica Set.

IV. üîê T·∫°o user v√† ph√¢n quy·ªÅn
mongosh
use admin
db.createUser({
  user: "admin",
  pwd: "StrongPass123",
  roles: [ { role: "root", db: "admin" } ]
})


Login l·∫°i:

mongosh -u admin -p StrongPass123 --authenticationDatabase admin

V. üß© Replica Set (High Availability)
C·∫•u h√¨nh 3 node:
Role	Host	Port
Primary	mongo1	27017
Secondary	mongo2	27017
Arbiter	mongo3	27017

Tr√™n Primary:

mongosh
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo1:27017" },
    { _id: 1, host: "mongo2:27017" },
    { _id: 2, host: "mongo3:27017", arbiterOnly: true }
  ]
})
rs.status()


T·ª± ƒë·ªông failover khi primary down.

Secondary c√≥ th·ªÉ ch·ªâ ƒë·ªçc (rs.slaveOk()).

VI. üßÆ Sharding (Scale-Out)

M·ª•c ti√™u: ph√¢n ph·ªëi d·ªØ li·ªáu theo ‚Äúshard key‚Äù ‚Üí chia t·∫£i gi·ªØa nhi·ªÅu node.

Th√†nh ph·∫ßn:

Config servers (mongod --configsvr)

Shard servers (mongod --shardsvr)

Query router (mongos)

L·ªánh c·∫•u h√¨nh:

mongosh
sh.enableSharding("appdb")
sh.shardCollection("appdb.users", { "userid": 1 })

VII. üíæ Backup & Restore
1Ô∏è‚É£ Dump to√†n b·ªô database
mongodump --uri="mongodb://admin:pass@localhost:27017" --out=/backup/mongo_$(date +%F)

2Ô∏è‚É£ Restore t·ª´ dump
mongorestore --uri="mongodb://admin:pass@localhost:27017" /backup/mongo_2025-11-01/

3Ô∏è‚É£ Snapshot (Filesystem-level)

N·∫øu d√πng LVM, Ceph, hay AWS EBS:

lvcreate --snapshot --name mongo_snap -L 10G /dev/vg0/mongo

VIII. üìä Monitoring MongoDB
D√πng l·ªánh CLI:
mongostat
mongotop

T√≠ch h·ª£p Prometheus Exporter:
docker run -d -p 9216:9216 bitnami/mongodb-exporter:latest \
  --mongodb.uri="mongodb://admin:pass@mongo1:27017/admin"


Metrics quan tr·ªçng:

mongodb_mongod_opLatencies_latency_microseconds_total

mongodb_mongod_connections_current

mongodb_mongod_mem_resident_megabytes

IX. üßØ Troubleshooting th·ª±c t·∫ø
S·ª± c·ªë	Nguy√™n nh√¢n	C√°ch x·ª≠ l√Ω
mongod kh√¥ng start	Port 27017 ƒëang b·ªã chi·∫øm	`netstat -tulnp
K·∫øt n·ªëi b·ªã t·ª´ ch·ªëi	bindIp sai ho·∫∑c ch∆∞a m·ªü firewall	ufw allow 27017
Secondary kh√¥ng sync	Network lag ho·∫∑c oplog qu√° nh·ªè	rs.printSlaveReplicationInfo()
Disk full	Log ho·∫∑c data chi·∫øm dung l∆∞·ª£ng	X√≥a log c≈©, d·ªçn /var/lib/mongo/journal
Performance k√©m	Index ch∆∞a t·ªëi ∆∞u	db.collection.getIndexes() v√† t·∫°o index ph√π h·ª£p
Auth failed	Sai DB auth	Lu√¥n ch·ªâ ƒë·ªãnh --authenticationDatabase admin
X. üß† Kinh nghi·ªám th·ª±c t·∫ø System Engineer

Lu√¥n b·∫≠t journaling (storage.journal.enabled: true) ƒë·ªÉ tr√°nh data loss.

D√πng WiredTiger engine (m·∫∑c ƒë·ªãnh) ‚Äì t·ªëi ∆∞u I/O v√† compression.

Oplog size ƒë·ªß l·ªõn cho replication (‚â•10% dung l∆∞·ª£ng d·ªØ li·ªáu).

Khi ch·∫°y container: mount persistent volume /data/db.

Backup ƒë·ªãnh k·ª≥ b·∫±ng mongodump v√† test kh√¥i ph·ª•c th·ª±c t·∫ø.

Gi√°m s√°t b·∫±ng Prometheus + Grafana dashboard (bitnami/mongodb).

Trong m√¥i tr∆∞·ªùng multi-site: replication async qua VPN/IPSec.

XI. ‚öôÔ∏è L·ªánh qu·∫£n tr·ªã nhanh (cheat sheet)
M·ª•c ti√™u	L·ªánh
Start mongod	systemctl start mongod
Ki·ªÉm tra tr·∫°ng th√°i replica	rs.status()
Xem log	tail -f /var/log/mongodb/mongod.log
T·∫°o DB m·ªõi	use newdb
Th√™m document	db.users.insert({name:"Alice", age:30})
Xem d·ªØ li·ªáu	db.users.find().pretty()
X√≥a DB	db.dropDatabase()
Export d·ªØ li·ªáu	mongoexport --db appdb --collection users --out users.json
XII. üîê Security Checklist

‚úÖ authorization: enabled trong c·∫•u h√¨nh
‚úÖ Kh√¥ng m·ªü 27017 ra Internet
‚úÖ D√πng user ri√™ng cho app, kh√¥ng d√πng root
‚úÖ SSL/TLS cho k·∫øt n·ªëi gi·ªØa nodes
‚úÖ Backup file keyFile ƒë·ªÉ ƒë·ªìng b·ªô auth gi·ªØa replica
‚úÖ Audit log b·∫≠t n·∫øu l√† m√¥i tr∆∞·ªùng Production

üß≠ MongoDB Operation Playbook

(D√†nh cho System Engineers ‚Äì Production Level)

I. üöÄ Cluster Initialization Checklist
1Ô∏è‚É£ Chu·∫©n b·ªã h·ªá th·ªëng

Y√™u c·∫ßu:

3 ho·∫∑c 5 node (s·ªë l·∫ª ƒë·ªÉ b·∫ßu primary)

OS: Ubuntu 22.04 LTS ho·∫∑c CentOS 8

M·ªói node: ‚â• 4GB RAM, SSD storage

ƒê·ªìng b·ªô th·ªùi gian: chrony ho·∫∑c ntpd

Firewall m·ªü port:

27017/tcp (mongod)
27018/tcp (mongos n·∫øu c√≥)
27019/tcp (config server)


C·∫•u tr√∫c m·∫´u:

Hostname	Role	IP
mongo1	Primary	10.0.0.1
mongo2	Secondary	10.0.0.2
mongo3	Arbiter	10.0.0.3
2Ô∏è‚É£ T·∫°o th∆∞ m·ª•c & c·∫•u h√¨nh c∆° b·∫£n
mkdir -p /data/db /var/log/mongodb
chown -R mongod:mongod /data /var/log/mongodb


/etc/mongod.conf:

storage:
  dbPath: /data/db
  journal:
    enabled: true
systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true
net:
  port: 27017
  bindIp: 0.0.0.0
security:
  keyFile: /etc/mongo-keyfile
  authorization: enabled
replication:
  replSetName: rs0
processManagement:
  fork: true

3Ô∏è‚É£ T·∫°o keyfile ƒë·ªÉ c√°c node x√°c th·ª±c l·∫´n nhau
openssl rand -base64 756 > /etc/mongo-keyfile
chmod 600 /etc/mongo-keyfile
chown mongod:mongod /etc/mongo-keyfile


Sao ch√©p file n√†y sang t·∫•t c·∫£ c√°c node.

4Ô∏è‚É£ Kh·ªüi t·∫°o Replica Set

Tr√™n mongo1:

mongosh
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo1:27017" },
    { _id: 1, host: "mongo2:27017" },
    { _id: 2, host: "mongo3:27017", arbiterOnly: true }
  ]
})


Ki·ªÉm tra:

rs.status()

II. üîê User & Access Control
1Ô∏è‚É£ T·∫°o user root
use admin
db.createUser({
  user: "admin",
  pwd: "StrongPass123!",
  roles: [ { role: "root", db: "admin" } ]
})

2Ô∏è‚É£ T·∫°o user cho ·ª©ng d·ª•ng
use appdb
db.createUser({
  user: "appuser",
  pwd: "App@2025!",
  roles: [ { role: "readWrite", db: "appdb" } ]
})

III. üßÆ Health Check Script

T·∫°o script /opt/mongodb/healthcheck.sh:

#!/bin/bash
mongo --quiet --eval 'rs.status().members.forEach(m => print(m.name + " : " + m.stateStr))'


T·ª± ƒë·ªông ch·∫°y m·ªói 5 ph√∫t qua cron:

*/5 * * * * /opt/mongodb/healthcheck.sh >> /var/log/mongo_health.log

IV. üíæ Backup & Restore Plan
1Ô∏è‚É£ Full Backup h√†ng ng√†y

/opt/mongodb/backup.sh

#!/bin/bash
DATE=$(date +%F)
BACKUP_DIR="/backup/mongo/$DATE"
mkdir -p $BACKUP_DIR

mongodump --uri="mongodb://admin:StrongPass123!@mongo1:27017/admin" --gzip --out=$BACKUP_DIR
find /backup/mongo/ -type d -mtime +7 -exec rm -rf {} \;   # X√≥a backup c≈© >7 ng√†y


Th√™m v√†o cron:

0 2 * * * /opt/mongodb/backup.sh

2Ô∏è‚É£ Restore
mongorestore --gzip --uri="mongodb://admin:StrongPass123!@mongo1:27017/admin" /backup/mongo/2025-11-01/

V. üìä Monitoring & Alerting
1Ô∏è‚É£ MongoDB Exporter (Prometheus)
docker run -d --name=mongo-exporter -p 9216:9216 \
  bitnami/mongodb-exporter:latest \
  --mongodb.uri="mongodb://admin:StrongPass123!@mongo1:27017/admin"

2Ô∏è‚É£ Dashboard Grafana (Metrics c·∫ßn gi√°m s√°t)
Metric	√ù nghƒ©a	Ng∆∞·ª°ng c·∫£nh b√°o
mongodb_up	Ki·ªÉm tra service s·ªëng	=1
mongodb_mongod_connections_current	S·ªë k·∫øt n·ªëi hi·ªán t·∫°i	< 80% max
mongodb_mongod_mem_resident_megabytes	RAM ti√™u th·ª•	Gi·ªõi h·∫°n 75%
mongodb_mongod_opLatencies_latency_microseconds_total	ƒê·ªô tr·ªÖ	< 5ms
mongodb_mongod_asserts_total	S·ªë l·ªói n·ªôi b·ªô	= 0
VI. üß∞ Common Maintenance Tasks
Nhi·ªám v·ª•	L·ªánh
Ki·ªÉm tra dung l∆∞·ª£ng collection	db.stats()
Compact d·ªØ li·ªáu	db.runCommand({compact:'collection'})
Check index	db.collection.getIndexes()
T·∫°o index m·ªõi	db.collection.createIndex({field:1})
Rotate log	logRotate
X√≥a d·ªØ li·ªáu c≈©	db.collection.remove({date:{$lt:ISODate("2024-01-01")}})
VII. üßØ Troubleshooting th·ª±c t·∫ø
V·∫•n ƒë·ªÅ	D·∫•u hi·ªáu	Nguy√™n nh√¢n	Gi·∫£i ph√°p
Primary down	Secondary ch·ªâ ƒë·ªçc	Network m·∫•t ho·∫∑c crash	Ki·ªÉm tra rs.status(), restart node
Replication delay	Secondary lag v√†i GB	Oplog nh·ªè, network ch·∫≠m	TƒÉng oplogSizeMB, ki·ªÉm tra bƒÉng th√¥ng
Disk full	mongod stop	Log ho·∫∑c d·ªØ li·ªáu qu√° l·ªõn	D·ªçn log, archive backup, m·ªü r·ªông volume
Auth failed	Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c	Sai DB ho·∫∑c credential	S·ª≠ d·ª•ng --authenticationDatabase admin
Performance k√©m	Query l√¢u	Thi·∫øu index	D√πng explain() ƒë·ªÉ t·ªëi ∆∞u index
Too many connections	K·∫øt n·ªëi full	App leak connection	Set maxIncomingConnections v√† pool limit
VIII. üåê Multi-site Replication

M·ª•c ti√™u: ƒë·∫£m b·∫£o HA/DR (High Availability / Disaster Recovery)

Site	Role	Node
DC1	Primary	mongo1, mongo2
DC2	Secondary	mongo3
DC3	Arbiter	mongo4

Thi·∫øt l·∫≠p:

rs.add({ host: "mongo3.dc2.local:27017", priority: 0, hidden: false })
rs.addArb("mongo4.dc3.local:27017")


Ch√∫ √Ω:

D√πng VPN ho·∫∑c private link gi·ªØa c√°c DC.

D√πng priority: 0 cho secondary ·ªü DR site (kh√¥ng ƒë∆∞·ª£c b·∫ßu primary tr·ª´ khi failover).

Gi√°m s√°t ping li√™n v√πng ƒë·ªÉ tr√°nh split-brain.

IX. ‚òÅÔ∏è Container & Kubernetes Integration

MongoDB StatefulSet (K8S) ‚Äì file mongodb.yaml (r√∫t g·ªçn):

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
spec:
  serviceName: "mongo"
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongod
        image: mongo:7.0
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: data
          mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 20Gi


Command ƒë·ªÉ join replica set:

kubectl exec -it mongo-0 -- mongosh --eval "rs.initiate({...})"

X. üß† Tips & Best Practices

D√πng WiredTiger storage engine (m·∫∑c ƒë·ªãnh).

ƒê·∫∑t journaling = true ‚Üí tr√°nh m·∫•t d·ªØ li·ªáu.

Backup tr∆∞·ªõc khi mongodump / restore / update version.

Kh√¥ng bao gi·ªù upgrade m√† ch∆∞a test tr√™n staging.

Lu√¥n test failover ƒë·ªãnh k·ª≥ (primary down ‚Üí auto election).

ƒê·∫∑t c·∫£nh b√°o Prometheus cho:

Replication lag > 60s

Disk > 80%

Connections > 80%

T√†i li·ªáu h√≥a to√†n b·ªô: IP, port, keyfile, account, backup path.

üß© MongoDB Disaster Recovery Plan (DRP)

M·ª•c ti√™u:
ƒê·∫£m b·∫£o d·ªØ li·ªáu MongoDB lu√¥n an to√†n, c√≥ th·ªÉ ph·ª•c h·ªìi v·ªõi RPO < 5 ph√∫t v√† RTO < 30 ph√∫t trong c√°c t√¨nh hu·ªëng m·∫•t m√°t ho·∫∑c gi√°n ƒëo·∫°n h·ªá th·ªëng.

I. üîç Ki·∫øn tr√∫c DR t·ªïng quan
1Ô∏è‚É£ M√¥ h√¨nh Multi-Site Replica Set (HA + DR)
Site	Vai tr√≤	Th√†nh ph·∫ßn
DC1 (Primary)	Production	mongo1, mongo2
DC2 (Secondary)	DR Site	mongo3
DC3 (Arbiter)	Tie-breaker	mongo4

Lu·ªìng replication:
Primary ‚Üí Secondary (DC2) ‚Üí Arbiter ch·ªâ tham gia b·∫ßu c·ª≠.
Replication async qua VPN ho·∫∑c private link.

2Ô∏è‚É£ C√°c th√¥ng s·ªë RTO / RPO ƒë·ªÅ xu·∫•t
Th√†nh ph·∫ßn	RPO (m·∫•t d·ªØ li·ªáu t·ªëi ƒëa)	RTO (th·ªùi gian kh√¥i ph·ª•c)
Replica Set	‚â§ 5 ph√∫t	‚â§ 30 ph√∫t
Backup full (mongodump)	24 gi·ªù	‚â§ 2 gi·ªù
Oplog	30 ph√∫t	‚â§ 10 ph√∫t
DR failover	0 ph√∫t (n·∫øu auto-election)	5‚Äì10 ph√∫t
II. ‚öôÔ∏è C·∫•u h√¨nh replication cho DR site

Tr√™n primary (mongo1):

rs.add({
  host: "mongo3.dr.local:27017",
  priority: 0,
  hidden: false,
  votes: 1
})
rs.addArb("mongo4.tie.local:27017")


üî∏ priority: 0 gi√∫p node DR kh√¥ng ƒë∆∞·ª£c b·∫ßu l√†m primary tr·ª´ khi th·ª±c s·ª± failover.
üî∏ N·∫øu mu·ªën DR site c√≥ th·ªÉ takeover trong s·ª± c·ªë l·ªõn ‚Üí n√¢ng priority: 1.

Ki·ªÉm tra tr·∫°ng th√°i:

rs.status()
rs.printReplicationInfo()

III. üíæ Chi·∫øn l∆∞·ª£c Backup & Restore DR
1Ô∏è‚É£ L·ªãch backup ƒë·ªãnh k·ª≥
Lo·∫°i	T·∫ßn su·∫•t	C√¥ng c·ª•	L∆∞u tr·ªØ
Full dump	H√†ng ng√†y (2AM)	mongodump	/backup/full/YYYY-MM-DD
Incremental	M·ªói 15 ph√∫t	oplog	/backup/oplog/YYYY-MM-DD/HHMM
Config backup	H√†ng tu·∫ßn	rs.conf()	/backup/config/
2Ô∏è‚É£ Backup script m·∫´u (full)

/opt/mongodb/backup_full.sh

#!/bin/bash
DATE=$(date +%F)
BACKUP_DIR="/backup/full/$DATE"
mkdir -p $BACKUP_DIR

mongodump --gzip --oplog --uri="mongodb://admin:StrongPass123!@mongo1:27017/admin" --out=$BACKUP_DIR
find /backup/full/ -type d -mtime +7 -exec rm -rf {} \;

3Ô∏è‚É£ Backup script (oplog incremental)

/opt/mongodb/backup_oplog.sh

#!/bin/bash
DATE=$(date +%F)
TIME=$(date +%H%M)
BACKUP_DIR="/backup/oplog/$DATE"
mkdir -p $BACKUP_DIR

mongodump --db local --collection oplog.rs --out=$BACKUP_DIR/$TIME --gzip
find /backup/oplog/ -type d -mtime +3 -exec rm -rf {} \;

4Ô∏è‚É£ Restore DR site t·ª´ backup
mongorestore --gzip --uri="mongodb://admin:StrongPass123!@mongo3:27017/admin" /backup/full/2025-11-01/

IV. üö® Quy tr√¨nh DR Failover
K·ªãch b·∫£n 1Ô∏è‚É£: Primary DC1 down (network ho·∫∑c hardware)
B∆∞·ªõc 1: Ki·ªÉm tra tr·∫°ng th√°i
rs.status()


N·∫øu mongo1 m·∫•t li√™n l·∫°c, mongo3 s·∫Ω kh√¥ng t·ª± b·∫ßu v√¨ priority=0.

B∆∞·ªõc 2: Promote secondary th√†nh primary

Tr√™n mongo3:

rs.stepUp()
rs.conf()


Ho·∫∑c ch·ªânh config:

cfg = rs.conf()
cfg.members[2].priority = 1
rs.reconfig(cfg, {force: true})

B∆∞·ªõc 3: Chuy·ªÉn ·ª©ng d·ª•ng sang DR endpoint

C·∫≠p nh·∫≠t connection string:

mongodb://mongo3.dr.local:27017, mongo4.tie.local:27017/?replicaSet=rs0

B∆∞·ªõc 4: Khi DC1 ph·ª•c h·ªìi

Reset l·∫°i priority

Sync d·ªØ li·ªáu t·ª´ DR ‚Üí DC1

rs.syncFrom("mongo3.dr.local:27017")

K·ªãch b·∫£n 2Ô∏è‚É£: Split-brain (hai site c√πng primary)

Nguy√™n nh√¢n: m·∫•t k·∫øt n·ªëi gi·ªØa site, arbiter unreachable.
Gi·∫£i ph√°p:

X√°c ƒë·ªãnh site h·ª£p l·ªá (∆∞u ti√™n DC1)

Shutdown mongod tr√™n site DR:

systemctl stop mongod


Sau khi link ·ªïn ƒë·ªãnh, re-sync th·ªß c√¥ng b·∫±ng initial sync:

rs.remove("mongo3.dr.local:27017")
rs.add("mongo3.dr.local:27017")

K·ªãch b·∫£n 3Ô∏è‚É£: Node b·ªã l·ªói d·ªØ li·ªáu (corruption, disk failure)

Stop mongod service:

systemctl stop mongod


X√≥a d·ªØ li·ªáu l·ªói:

rm -rf /data/db/*


Re-sync l·∫°i t·ª´ node kh√°c:

rs.add("mongo3.dr.local:27017")

V. üîé Ki·ªÉm th·ª≠ DR ƒë·ªãnh k·ª≥

Chu k·ª≥: 1 l·∫ßn / th√°ng
N·ªôi dung ki·ªÉm th·ª≠:

Ki·ªÉm th·ª≠	K·∫øt qu·∫£ mong ƒë·ª£i
T·∫Øt primary	DR takeover < 2 ph√∫t
Restore backup full	Restore th√†nh c√¥ng, checksum OK
Test restore oplog	D·ªØ li·ªáu kh·ªõp RPO
Test split-brain	Cluster gi·ªØ consistency
Test network latency	< 100ms gi·ªØa site

Command ki·ªÉm th·ª≠:

mongo --eval 'printjson(rs.status())'
mongostat --host mongo3.dr.local --rowcount 5

VI. üß† DR Automation (Ansible + Bash)

roles/mongodb-dr/tasks/main.yml

- name: Backup MongoDB full dump
  command: bash /opt/mongodb/backup_full.sh

- name: Verify replication lag
  shell: "mongo --eval 'rs.printSlaveReplicationInfo()'"
  register: repl_info
  changed_when: false

- name: Failover if primary down
  shell: "mongo --eval 'rs.stepUp()'"
  when: "'PRIMARY' not in repl_info.stdout"

VII. üìà Monitoring DR Replication

Th√™m v√†o Prometheus rules:

- alert: MongoReplicationLag
  expr: mongodb_mongod_replset_oplog_head_timestamp - mongodb_mongod_replset_member_optime_timestamp > 60
  for: 5m
  labels:
    severity: warning
  annotations:
    description: "Replication lag > 60s"


Grafana dashboard: theo d√µi Replication Lag, Member State, Election Count.

VIII. üìò T√†i li·ªáu & B√°o c√°o

DR Runbook: /docs/DRP_MongoDB_v1.2.pdf

Last DR Test: /reports/DR_Test_Results_2025-10.xlsx

Backup Verification Log: /logs/backup_verify.log

Contact matrix:

DBA on-call

Network admin

System engineer (24/7)

IX. ‚úÖ Best Practices t·ªïng h·ª£p

Replica ·ªü DR site n√™n priority=0 (ch·ªâ b·∫≠t khi failover)

Lu√¥n ki·ªÉm tra replication lag < RPO

Backup ƒë·ªãnh k·ª≥: full + oplog

S·ª≠ d·ª•ng monitoring t·ª± ƒë·ªông (Prometheus, Grafana)

C·∫£nh b√°o khi:

Lag > 60s

Disk > 80%

Node state != PRIMARY/SECONDARY

T√†i li·ªáu h√≥a:

Cluster topology

Backup schedule

DR recovery steps

